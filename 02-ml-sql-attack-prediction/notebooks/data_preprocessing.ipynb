{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fbdd138",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Extracted preprocessing steps from `project pipeline.ipynb`.\n",
    "Ensure your GTD Excel file is uploaded to the path shown in the `base_path` cell before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b72e8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy matplotlib seaborn scikit-learn xgboost sqlalchemy plotly joblib -q\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")\n",
    "print(\"Setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890c813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# NOTE: On non-Colab environments, remove or adapt the above drive.mount call and ensure `base_path` points to a local location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb98dd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/content/drive/MyDrive/LABS/Project-1-GTD-Analytics'\n",
    "os.makedirs(f'{base_path}/data', exist_ok=True)\n",
    "os.makedirs(f'{base_path}/dashboards', exist_ok=True)\n",
    "os.makedirs(f'{base_path}/sql', exist_ok=True)\n",
    "\n",
    "print(f\"Ready at: {base_path}\")\n",
    "print(\"Upload the GTD .xlsx to: /content/drive/MyDrive/LABS/Project-1-GTD-Analytics/data/gtd_dataset.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddfb91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "gtd_path = f'{base_path}/data/Copy of GTD'\n",
    "df = pd.read_excel(gtd_path)\n",
    "print(f\"Loaded {df.shape[0]:,} attacks Ã— {df.shape[1]} columns\")\n",
    "print(\"\n",
    "Key columns found:\")\n",
    "print(df.columns.tolist()[:20])\n",
    "print(\"\n",
    "Sample data:\")\n",
    "df[['iyear', 'imonth', 'iday', 'country_txt', 'region_txt', 'attacktype1_txt',\n",
    "    'nkill', 'nwound']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa448537",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"MASTER ETL PIPELINE\")\n",
    "\n",
    "# Start with raw data\n",
    "df_etl = df.copy()\n",
    "\n",
    "# STEP 1: Select columns\n",
    "cols = ['eventid', 'iyear', 'imonth', 'iday', 'country_txt', 'region_txt',\n",
    "        'city', 'latitude', 'longitude', 'attacktype1_txt', 'targtype1_txt',\n",
    "        'weaptype1_txt', 'nkill', 'nwound', 'success']\n",
    "available_cols = [col for col in cols if col in df.columns]\n",
    "df_etl = df_etl[available_cols].copy()\n",
    "\n",
    "# STEP 2: Convert ALL to SQLite-safe types FIRST\n",
    "for col in ['iyear', 'imonth', 'iday', 'nkill', 'nwound', 'success']:\n",
    "    if col in df_etl.columns:\n",
    "        df_etl[col] = pd.to_numeric(df_etl[col], errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "for col in ['latitude', 'longitude']:\n",
    "    if col in df_etl.columns:\n",
    "        df_etl[col] = pd.to_numeric(df_etl[col], errors='coerce').fillna(0)\n",
    "\n",
    "if 'city' in df_etl.columns:\n",
    "    df_etl['city'] = df_etl['city'].fillna('Unknown')\n",
    "\n",
    "# STEP 3: Create SQLite-safe date strings (NO Period objects)\n",
    "df_etl['year_month'] = df_etl['iyear'].astype(str) + '-' + df_etl['imonth'].astype(str).str.zfill(2)\n",
    "df_etl['date_str'] = (df_etl['iyear'].astype(str) + '-' +\n",
    "                     df_etl['imonth'].astype(str).str.zfill(2) + '-' +\n",
    "                     df_etl['iday'].astype(str).str.zfill(2))\n",
    "\n",
    "# STEP 4: Features (all numeric/string)\n",
    "df_etl['casualties'] = df_etl['nkill'] + df_etl['nwound']\n",
    "df_etl['fatality_rate'] = df_etl['nkill'] / (df_etl['casualties'] + 1)\n",
    "\n",
    "# STEP 5: Severity (string only)\n",
    "df_etl['severity'] = pd.cut(df_etl['casualties'],\n",
    "                            bins=[0, 1, 10, 50, np.inf],\n",
    "                            labels=['Low', 'Medium', 'High', 'Extreme']).astype(str)\n",
    "\n",
    "# SAVE CLEAN VERSION for charts/ML\n",
    "df_clean = df_etl.copy()\n",
    "df_clean.to_csv(f'{base_path}/data/gtd_cleaned.csv', index=False)\n",
    "\n",
    "print(f\"SUCCESS! ETL complete:\")\n",
    "print(f\"- {len(df_etl):,} rows processed\")\n",
    "print(f\"- df_clean saved for charts/ML\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3faca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: load the saved cleaned CSV\n",
    "df_clean = pd.read_csv(f'{base_path}/data/gtd_cleaned.csv')\n",
    "print('Cleaned data loaded:', df_clean.shape)\n",
    "df_clean.head()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
